- name: Ensure nodes have public IP labels
  when: "'control_plane' in group_names"
  block:
      # Create an ansible variable that is a list of hostnames (not as reached by ssh but as they appear in the cluster)
      # which have a public IP address
    - name: Get list of nodes that have a public IP
      ansible.builtin.set_fact:
        nodes_with_public_IP: "{{ hostvars | dict2items |
        map(attribute='value') |
        selectattr('public_network_address', 'defined') |
        map(attribute='ansible_hostname') }}"

      # On the control plane node, run this command which will add a label to the kubernetes Node object for each host
      # that has a public IP address
    - name: Apply labels to those nodes
      loop: "{{ nodes_with_public_IP }}"
      ansible.builtin.shell: |
        kubectl label nodes {{ item }} hasPublicIP="true"

- name: Check ingress installed
  when: "'control_plane' in group_names"
  ansible.builtin.shell: kubectl get deployments -n ingress-nginx
  failed_when: False
  changed_when: False
  register: ingress_deployments

- name: Install ingress
  when:
    - "'control_plane' in group_names"
    - ingress_deployments.stdout_lines | count < 1
  block:
      # helm repo add
    - name: Install ingress, add helm repo (1/3)
      kubernetes.core.helm_repository:
        name: k8s-ingress-nginx
        repo_url: "https://kubernetes.github.io/ingress-nginx"

    - name: Install ingress, copy values (2/3)
      ansible.builtin.template:
        src: "{{ playbook_dir }}/install/templates/ingress_helm_values.yaml.j2"
        dest: /etc/kubernetes/helm_values/ingress.yaml
        owner: root
        group: root
        mode: 0640

      # helm install -f /etc/kubernetes... ingress-nginx k8s-ingress-nginx/ingress-nginx
      # --namespace ingress-nginx --create-namespace
      # and then wait
    - name: Install ingress, install chart (3/3)
      kubernetes.core.helm:
        update_repo_cache: True
        release_name: ingress-nginx
        release_namespace: ingress-nginx
        create_namespace: True
        chart_ref: k8s-ingress-nginx/ingress-nginx
        atomic: True
        values_files:
          - /etc/kubernetes/helm_values/ingress.yaml

- name: Check cert_manager installed
  when: "'control_plane' in group_names"
  ansible.builtin.shell: kubectl get deployments -n cert-manager
  failed_when: False
  changed_when: False
  register: cert_manager_deployments

- name: Install cert_manager
  when:
    - "'control_plane' in group_names"
    - cert_manager_deployments.stdout_lines | count < 1
  block:
      # helm repo add
    - name: Install cert_manager, add helm repo (1/3)
      kubernetes.core.helm_repository:
        name: jetstack
        repo_url: "https://charts.jetstack.io"

    - name: Install cert_manager, copy values (2/3)
      ansible.builtin.template:
        src: "{{ playbook_dir }}/install/templates/cert_manager_helm_values.yaml.j2"
        dest: /etc/kubernetes/helm_values/cert_manager.yaml
        owner: root
        group: root
        mode: 0640

      # helm install, like above for ingress-nginx
    - name: Install cert_manager, install chart (3/3)
      kubernetes.core.helm:
        update_repo_cache: True
        release_name: cert-manager
        release_namespace: cert-manager
        create_namespace: True
        chart_ref: jetstack/cert-manager
        atomic: True
        values_files:
          - /etc/kubernetes/helm_values/cert_manager.yaml

- name: Check cluster issuer installed
  when: "'control_plane' in group_names"
  ansible.builtin.shell: kubectl get clusterissuer
  failed_when: False
  changed_when: False
  register: cluster_issuers

- name: Create cluster issuer
  when:
    - "'control_plane' in group_names"
    - cluster_issuers.stdout_lines | count < 1
  block:
    - name: Install cert_manager, copy issuer manifest (1/2)
      ansible.builtin.template:
        src: "{{ playbook_dir }}/install/templates/cluster-issuer.yaml.j2"
        dest: /etc/kubernetes/custom_manifests/cluster-issuer.yaml

      # kubectl apply -f /etc/kubernetes...
      # and then wait
    - name: Install cert_manager, apply issuer manifest (2/2)
      kubernetes.core.k8s:
        kubeconfig: "{{ k8s_admin_config }}"
        apply: True
        src: /etc/kubernetes/custom_manifests/cluster-issuer.yaml
        wait: True

  # The teardown playbook saves both secrets containing TLS certificates (to avoid letsencrypt ratelimits)
  # and the registration for the acme-dns server (necessary to keep the same subdomain strings as before).
  # This checks if the expected secrets file is there, and if so applies it in the following steps
- name: Check if certificate secrets were saved from a previous cluster
  when: "'control_plane' in group_names"
  ansible.builtin.shell: ls /root/kubernetesbackup/backupcerts.yaml
  failed_when: false
  register: certificate_secrets_saved

- name: Reinstall certificate secrets if saved from a previous cluster
  when:
    - "'control_plane' in group_names"
    - certificate_secrets_saved.stdout_lines | count > 0
  block:
    - name: Get the namespaces where there are secrets
      ansible.builtin.shell: >
        cat /root/kubernetesbackup/backupcerts.yaml | yq e '.metadata.namespace' | grep -v -- ---
      register: secrets_namespaces

      # kubectl create namespace ...
    - name: Create the namespaces for the secrets
      loop: "{{ secrets_namespaces.stdout_lines }}"
      kubernetes.core.k8s:
        kubeconfig: "{{ k8s_admin_config }}"
        name: "{{ item }}"
        state: present
        kind: Namespace
        api_version: v1


      # kubectl apply -f
    - name: Apply the manifest if saved
      kubernetes.core.k8s:
        kubeconfig: "{{ k8s_admin_config }}"
        apply: True
        src: /root/kubernetesbackup/backupcerts.yaml
        wait: True
