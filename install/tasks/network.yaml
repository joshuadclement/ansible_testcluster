- name: Configure IP pools
  tags:
    - ip_pools
    - network
  when:
    - "'control_plane' in group_names"
  block:
    # There are two goals.
    # 1) Create an IP pool for the vlan subnet for traffic to the silos from the pods
    # so that source network address translation won't be performed on traffic to the silos.
    # see https://projectcalico.docs.tigera.io/networking/workloads-outside-cluster
    # 2) Create a separate IP pool for each node,
    # so that a short list of static routes can be applied to the silos to route traffic back to the pods.
    # see https://projectcalico.docs.tigera.io/networking/assign-ip-addresses-topology

    - name: Configure IP pools, wait until calico apiserver is ready (1/6)
      kubernetes.core.k8s:
        wait: True
        wait_condition:
          type: Available
          status: "True"
        api_version: v1
        kind: Deployment
        name: calico-apiserver
        namespace: calico-apiserver
        state: present

    - name: Configure IP pools, delete default pool (2/6)
      kubernetes.core.k8s:
        kubeconfig: "{{ k8s_admin_config }}"
        wait: True
        state: absent
        name: default-ipv4-ippool
        kind: IPPool
        api_version: projectcalico.org/v3

    - name: Configure IP pools, copy manifest (3/6)
      ansible.builtin.template:
        src: "{{ playbook_dir }}/install/templates/calico_ip_pools.yaml.j2"
        dest: /etc/kubernetes/custom_manifests/calico_ip_pools.yaml
        owner: root
        group: root
        mode: 0640

    - name: Configure IP pools, apply manifest (4/6)
      kubernetes.core.k8s:
        kubeconfig: "{{ k8s_admin_config }}"
        apply: True
        src: /etc/kubernetes/custom_manifests/calico_ip_pools.yaml
        wait: True

      # Some pods will have an IP address of its own host node, and others will have a cluster IP.
      # All that should exist at this point should be system components that are deployments or daemon sets,
      # so deleting them will have new ones rescheduled in the newly configured IPPools
    - name: Configure IP pools, get names of pods with cluster subnet IP addresses for deletion (5/6)
      ansible.builtin.shell: \
        kubectl get pods --all-namespaces -o wide |
        {% for host in groups.cluster %} grep -v {{ hostvars[host]['trusted_network_address'] }} |{% endfor %}
         grep Running | awk '{print $1 " " $2}'
      register: pods_to_delete

    - name: Configure IP pools, delete pods scheduled with the default IPPool (6/6)
      loop: "{{ pods_to_delete.stdout_lines }}"
      kubernetes.core.k8s:
        kubeconfig: "{{ k8s_admin_config }}"
        wait: True
        name: "{{ item | split | last }}"
        namespace: "{{ item | split | first }}"
        state: absent
        api_version: v1
        kind: Pod

- name: Reboot after configuring IP pools
  tags:
    - ip_pools
    - network
  ansible.builtin.reboot:

- name: Apply network policies
  tags:
    - network
    - network_policies
  when:
    - "'control_plane' in group_names"
  block:
    - name: Apply network policies, copy manifest (1/5)
      ansible.builtin.template:
        src: "{{ playbook_dir }}/install/templates/network_policies.yaml.j2"
        dest: /etc/kubernetes/custom_manifests/network_policies.yaml
        owner: root
        group: root
        mode: 0640

    - name: Apply network policies, create namespaces where policies should apply (2/5)
      loop: ["sciencedata", "sciencedata-dev"]
      kubernetes.core.k8s:
        kubeconfig: "{{ k8s_admin_config }}"
        name: "{{ item }}"
        kind: Namespace
        state: present
        api_version: v1

    - name: Apply network policies, wait until calico deployments are ready after reboot (3/5)
      loop: [{"namespace": "calico-apiserver", "name": "calico-apiserver"}, {"namespace": "calico-system", "name": "calico-kube-controllers"}, {"namespace": "calico-system", "name": "calico-typha"}]
      kubernetes.core.k8s:
        wait: True
        wait_condition:
          type: Available
          status: "True"
        api_version: v1
        kind: Deployment
        name: "{{ item.name }}"
        namespace: "{{ item.namespace }}"
        state: present

    - name: Apply network policies, sleep for good measure (not sure what needs to get ready before this will work) (4/5)
      ansible.builtin.shell: sleep 45

    - name: Apply network policies, apply manifest (4/5)
      kubernetes.core.k8s:
        kubeconfig: "{{ k8s_admin_config }}"
        apply: True
        src: /etc/kubernetes/custom_manifests/network_policies.yaml
        wait: True

